---
title: "Ant data: simulating data to solidify understanding and test algorithms"
author: Brett Melbourne
date: 4 Dec 2021
output:
    github_document:
        pandoc_args: --webtex
---

```{r, results=FALSE, message=FALSE, warning=FALSE}
library(lme4)
library(ggplot2)
library(rstanarm)
options(mc.cores = parallel::detectCores())
theme_set(theme_grey())
```

This is the fifth in a series of scripts to analyze the ant data described in Ellison (2004). In this script we're going to generate simulated data to solidify our understanding of the model (the data generating process) and to check that our model, training, and inference algorithms are working as intended.

We'll follow the general approach outlined in [15_2_sim_multilevel.Rmd](15_2_sim_multilevel.md) but this time for the specific case of the ant data. We don't want to get too complicated right away, so we'll pick one of the simpler models we've been entertaining. We can increase complexity gradually as we need to. Starting simple and building complexity is a good modeling strategy. Thoroughly understanding a simpler model first will help us understand more complex models as we build them.

We want to simulate the response variable (species richness) as a function of the predictors. One of our most basic models is a generalized linear model with a Poisson distribution for species richness and log link with latitude and habitat type as linear predictors. We also have a random effect of site. A site is a pair of sampling plots, each plot with a different habitat type. We entertained this model earlier in [12_3_ants_multilevel.R](12_3_ants_multilevel.md). We previously looked in detail at the mathematical model in [12_4_slides_wed_ants_GLMM.pdf](12_4_slides_wed_ants_GLMM.pdf). The hierarchical form of this model (with some minor notation changes to improve clarity) is:

$y_i \sim \mathrm{Poisson}(\mu_i)$ \
$\mathrm{ln}(\mu_i) = \alpha_{j[i]} + \beta_1 \mathrm{forest}_i + \beta_3 \mathrm{forest}_i \times \mathrm{latitude}_{j[i]}$\
$\alpha_j \sim \mathrm{Normal}(\mathrm{ln}(\mu_j),\sigma_{\alpha}^2)$\
$\mathrm{ln}(\mu_j) = \beta_0 + \beta_2 \mathrm{latitude}_j$\

It would be simpler to simulate the additive decomposition form of this model (see [12_4_slides_wed_ants_GLMM.pdf](12_4_slides_wed_ants_GLMM.pdf)) because it collapses some of the data generating processes into an equivalent but simpler form. We're going to use the above hierarchical form of the model here because it is a more literal expression of our concept for how the data are generated step by step, and so simulating it will consolidate our understanding of our biologically-inspired theory (or *story*) of the data generating process. In other words, the data simulation will help us to think through the question of "does my model tell my story for how I imagine the biology?". To simulate data for this model we just need to implement it as an algorithm in code. 

The equations broadly say that there are four steps to generating species richness, starting from line 4 of the equations and working back to line 1. In pseudocode, our data story is as follows:

```
for each site j
    latitude determines broad-scale richness (Eq. line 4)
    but there is some stochasticity about this (Eq. line 3)
    
for each plot i
    habitat modifies local richness (Eq. line 2)
    then stochasticity determines the final richness (Eq. line 1)
```

So, we have a hierarchical and step-by-step concept for how biological processes at different scales ultimately give rise to the number of ant species at the scale of individual data points (plots). First, latitude determines the big picture, then it is modified by habitat at smaller scales, and stochasticity is important at both large and small scales.

To generate data then, broadly our data generating algorithm is:

```
for each site j
    generate an expected ln(richness) based on latitude (Eq. line 4)
    generate stochasticity around this expectation (Eq. line 3)
    
for each plot i
    generate an expected ln(richness) based on habitat type (Eq. line 2)
    generate richness with stochasticity (Eq. line 1)
```

The data are generated separately at the different scales, reflecting our biological story.

Our intention is to match the study design of our real dataset, so we want to use the predictor variables as they are in the real dataset:

```{r}
ant <- read.csv("data/ants.csv")
```

Recall that we have 22 sites, each with an associated latitude, and at each site we have both a forest and a bog sample plot:

```{r}
ant[,c("site", "habitat", "latitude")]
```

We're going to need some parameter values for the model. We could make them up entirely, base them on prior experience or biological plausibility, or base them on our real data. Here, from a plot of the real data, I eyeballed mean richness values at latitudes 42 and 45 for bog habitat to give these equations for the linear predictor (from line 4 of the mathematical model equation above):

$\mathrm{ln}(6) = \beta_0 + \beta_2 \times 42$\
$\mathrm{ln}(3) = \beta_0 + \beta_2 \times 45$

which I then solved by hand (2 equations, 2 unknowns) to give reasonable values for $\beta_0$ and $\beta_2$. I did the same for forest habitat, where I discovered the slope was about the same (i.e. $\beta_3$ = 0). For $\sigma_{\alpha}$ I thought 25% stochasticity was a good place to start based on my knowledge of ecological processes; given the ln scale, that's about 0.25. We could alternatively have used estimates from the fit of the model we obtained in [12_3_ants_multilevel.R](12_3_ants_multilevel.md) but we don't have to fit a model to real data at this stage.

Now, let's translate our pseudocode into R code, adding the necessary detail from the equations plus some infrastructure (parameter values and data structures), as well as vectorizing the for loops from our pseudocode.

```{r}
# Parameters
b_0 <- 11.5
b_1 <- 0.7
b_2 <- -0.231
b_3 <- 0
sigma_alpha <- 0.25

# Set up the study design the same as the real data:

# Site-scale variable (22 sites)
latitude <- ant$latitude[1:22]

# Plot-scale variables (44 plots, 2 per site)
j <- rep(1:22, 2) #site ID
forest <- ifelse(ant$habitat == "forest", 1, 0) #forest indicator

# Generate data:

# For each site, generate an expected ln(richness) based on latitude (Eq. line 4)
mu_alpha <- b_0 + b_2 * latitude

# For each site, generate stochasticity around this expectation (Eq. line 3)
# (note how this value will be the same for both plots at a site)
alpha <- rnorm(22, mu_alpha, sigma_alpha)

# For each plot, generate an expected ln(richness) based on habitat type (Eq. line 2)
# (we use j to extract the appropriate alpha and latitude values)
ln_mu <- alpha[j] + b_1 * forest + b_3 * forest * latitude[j]

# For each plot, generate richness with stochasticity (Eq. line 1)
# (we use the inverse link function to obtain mu)
y <- rpois(44, exp(ln_mu))

# Put in a data frame
sim_dat <- cbind(ant[,1:3], richness=y)

```

How do the simulated data look?

```{r}
ggplot(data=sim_dat, mapping=aes(x=latitude, y=richness, col=habitat)) +
    geom_point()
```

Not bad! It's even quite like the real data. Now, if we fit the model to these simulated data, can we recover the known parameters (i.e. the parameter values we coded in)?

```{r}
sim_dat$habitat <- factor(sim_dat$habitat)
sim_dat$site <- factor(sim_dat$site)
bayesHxL <- stan_glmer(richness ~ habitat + latitude + habitat:latitude + (1|site), 
                       family=poisson, data=sim_dat)
print(summary(bayesHxL)[,c("mean","sd","n_eff","Rhat")], digits=3)
```
Comparing the relevant parameters in turn, we see that the estimated values are close to the known values we coded in.

```{r}
cbind(b_0, estimated=summary(bayesHxL)["(Intercept)","mean"])
cbind(b_1, estimated=summary(bayesHxL)["habitatforest","mean"])
cbind(b_2, estimated=summary(bayesHxL)["latitude","mean"])
cbind(b_3, estimated=summary(bayesHxL)["habitatforest:latitude","mean"])
cbind(sigma_alpha, estimated=sqrt(summary(bayesHxL)["Sigma[site:(Intercept),(Intercept)]","mean"]))
            
```

This gives us confidence that 1) we understand our model (our intended data generating process), 2) we've correctly translated the model into `stan_glmer` syntax, and 3) the algorithms for training and inference are working as expected. We should run the above code a number of times to get a feeling for the variability in the data generating process and how consistently the estimated parameters from model fitting compare to the true values. The model fit from a single realization is not necessarily going to be close to the known parameters.

We can do many things from here. We can try different parameter values to test the model fitting or to explore how parameter values determine the data generating process. For example, how does site scale stochasticity influence the uncertainty of species richness at the plot scale? We could add complexity to the model, for example by adding new variables, or overdispersion. We could turn the model code into a function so that we can easily change parameter values. We could run the model many times and use a facet plot to visualize the variability in stochastic realizations of the data generating process and the resulting model fits. Simulating data is a powerful approach to clarify our scientific thinking, build our understanding of our model and quantitative methods, and to test our code and algorithms.
